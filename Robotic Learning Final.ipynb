{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from gym.envs.classic_control import rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def repeat_upsample(rgb_array, k=1, l=1, err=[]):\n",
    "    # repeat kinda crashes if k/l are zero\n",
    "    if k <= 0 or l <= 0: \n",
    "        if not err: \n",
    "            print(\"Number of repeats must be larger than 0, k: {}, l: {}, returning default array!\".format(k, l))\n",
    "            err.append('logged')\n",
    "        return rgb_array\n",
    "\n",
    "    # repeat the pixels k times along the y axis and l times along the x axis\n",
    "    # if the input image is of shape (m,n,3), the output image will be of shape (k*m, l*n, 3)\n",
    "\n",
    "    return np.repeat(np.repeat(rgb_array, k, axis=0), l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "viewer_S = rendering.SimpleImageViewer()\n",
    "viewer_A = rendering.SimpleImageViewer()\n",
    "viewer_D = rendering.SimpleImageViewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_S = gym.make('SpaceInvaders-v0')\n",
    "print(\"Action Space: \", env_S.action_space)\n",
    "print(\"Observation Space: \", env_S.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(7)\n",
      "Observation Space:  Box(250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_A = gym.make('Assault-v0')\n",
    "print(\"Action Space: \", env_A.action_space)\n",
    "print(\"Observation Space: \", env_A.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_D = gym.make('DemonAttack-v0')\n",
    "print(\"Action Space: \", env_D.action_space)\n",
    "print(\"Observation Space: \", env_D.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent_all():\n",
    "    def __init__(self, env):\n",
    "        # 0 = nothing, 1 = fire, 2 = right, 3 = left\n",
    "        self.common_actions = [0, 1, 2, 3]\n",
    "        self.assult_translation = {0:1, 1:2, 2:3, 3:4}\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        base_action = random.choice(self.common_actions)\n",
    "        \n",
    "        assault_action = self.assult_translation[base_action]\n",
    "        \n",
    "        return base_action, base_action, assault_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_S = env_S.reset()\n",
    "state_A = env_A.reset()\n",
    "state_D = env_D.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent = Agent_all(env_S)\n",
    "\n",
    "for _ in range(2000):\n",
    "    action_S, action_D, action_A = agent.get_action(state_S)\n",
    "    \n",
    "    state_S, reward_S, done_S, info_S = env_S.step(action_S)\n",
    "    \n",
    "    rgb_S = env_S.render('rgb_array')\n",
    "    rgb_S = rgb_S[22:195, :, :]\n",
    "    viewer_S.imshow(repeat_upsample(rgb_S, 3, 3))\n",
    "    \n",
    "    state_A, reward_A, done_A, info_A = env_A.step(action_A)\n",
    "    \n",
    "    rgb_A = env_A.render('rgb_array')\n",
    "    rgb_A = rgb_A[51:195+29, :, :]\n",
    "    viewer_A.imshow(repeat_upsample(rgb_A, 3, 3))\n",
    "    \n",
    "    state_D, reward_D, done_D, info_D = env_D.step(action_D)\n",
    "    \n",
    "    rgb_D = env_D.render('rgb_array')\n",
    "    rgb_D = rgb_D[15:188, :, :]\n",
    "    viewer_D.imshow(repeat_upsample(rgb_D, 3, 3))\n",
    "    \n",
    "    #time.sleep(0.1)\n",
    "    if done_S & done_A & done_D:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 160)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_A.mean(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SI: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  \n",
    "A:  0 = nothing, 1 = nothing?, 2 = fire, 3 = right, 4 = left, 5 = shoot right, 6 = shoot left  \n",
    "DA: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tutorial\n",
    "Full code, but using tensorflow\n",
    " - https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb  \n",
    " - https://www.youtube.com/watch?v=gCJyVX98KJ4\n",
    "\n",
    "Using Pytorch\n",
    " - https://www.youtube.com/watch?v=RfNxXlO6BiA\n",
    "\n",
    "Also\n",
    " - https://www.youtube.com/watch?v=dpBKz1wxE_c\n",
    " \n",
    "Frame Skipping and Stacking\n",
    " - https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    " \n",
    "General DQN Walkthrough =\n",
    " - https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray # Help us to gray our frames\n",
    "\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "print(\"Action Space: \", env.action_space)\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
    "#possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "# - His env takes a one hot vector as the action. Ours just takes an integer\n",
    "\n",
    "possible_actions = range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame \n",
    "    gray = rgb2gray(frame)\n",
    "    \n",
    "    # Crop the screen (remove the part below the player)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = gray[8:-12,4:-12]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    # Thanks to Mikołaj Walkowiak\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
    "    \n",
    "    return preprocessed_frame # 110x84x1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode, stack_size=4):\n",
    "    # Preprocess frame\n",
    "    # https://github.com/openai/gym/issues/275 - Env is already skipping frames\n",
    "    # Stacked_frames = the queue, stacked_state = an array of the same information\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for i in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "        \n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size, consecutive=False):\n",
    "        buffer_size = len(self.buffer)\n",
    "        \n",
    "        if consecutive:\n",
    "            rand_start = np.random.choice(range(buffer_size - batch_size - 1))\n",
    "            sample = self.buffer[rand_start:rand_start + batch_size]\n",
    "        else:\n",
    "            random_indices = np.random.choice(np.arange(buffer_size),\n",
    "                                    size = batch_size,\n",
    "                                    replace = False)\n",
    "            sample = [self.buffer[i] for i in random_indices]\n",
    "        \n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    \"\"\"\n",
    "    Number of Linear input connections depends on output of conv2d layers\n",
    "    and therefore the input image size, so compute it.\n",
    "    \"\"\"\n",
    "    out_size = (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "    return out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, frame_shape, num_outputs, alpha):\n",
    "        super(DQN, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        h, w = frame_shape\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        self.linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_outputs)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization.\n",
    "    def forward(self, x):\n",
    "        batch_shape = x.shape\n",
    "        x = T.Tensor(x).to(self.device)\n",
    "        \n",
    "        # Must transpose it - comes in as (h, w, frames). Change to (frames, h, w)\n",
    "        x = x.view(batch_shape[0], batch_shape[3], batch_shape[1], batch_shape[2])\n",
    "\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten \n",
    "        x = x.view(-1, self.linear_input_size)\n",
    "        \n",
    "        # Linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_vals = F.relu(self.fc2(x))\n",
    "        \n",
    "        # q_vals is a q value for each action for each item in minibatch (Eg: 32 * 4)\n",
    "        \n",
    "        return q_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, frame_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "                 alpha, max_memory, replace, action_space):\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon_start = epsilon_start # For greedy action selection\n",
    "        self.epsilon_end = epsilon_end #How low epsilon can go\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.replace = replace # How often to replace target network\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0 #How many times we've called learn function - For target network replacement\n",
    "        self.memory = Memory(max_memory)\n",
    "        \n",
    "        self.replace_target = replace\n",
    "        \n",
    "        self.Q_eval = DQN(frame_shape, len(action_space), alpha) # Agent's estimate of current set of states' Q\n",
    "        self.Q_next = DQN(frame_shape, len(action_space), alpha) # Agent's estimage of next set of states' Q\n",
    "        \n",
    "        \n",
    "    def storeTransition(self, state, action, reward, result_state):\n",
    "        \"\"\"\n",
    "        Wrap the inputs into a \"Transition\" and puts in in memory \n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, result_state)\n",
    "        self.memory.add(transition)\n",
    "        \n",
    "        \n",
    "    def chooseAction(self, observation):\n",
    "        \"\"\"\n",
    "        Epsilon Greedy strategy with decaying exploration prob.\n",
    "        \"\"\"\n",
    "        rand = np.random.rand()\n",
    "        \n",
    "        # Lower the exploration probability over time in accordance with \n",
    "        #  the number of times the model has learned \n",
    "        explore_probability = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                              np.exp(-self.epsilon_decay * self.learn_step_counter)\n",
    "        \n",
    "        if rand < explore_probability:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            actions_pred = self.Q_eval.forward(observation)\n",
    "                 # Take first axis because actions returned as a matrix\n",
    "            action = T.argmax(actions_pred[1]).item()\n",
    "            \n",
    "        self.steps += 1\n",
    "            \n",
    "        return action, explore_probability\n",
    "    \n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        \"\"\"\n",
    "        Batch learn\n",
    "        \"\"\"\n",
    "        # Zero gradients for next batch\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        # Target network replacement\n",
    "        if (self.replace_target is not None) & (self.learn_step_counter % self.replace_target == 0):\n",
    "            print('replacing target')\n",
    "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "            \n",
    "        # Get a minibatch\n",
    "        minibatch = self.memory.sample(batch_size, consecutive=False) # Not sure they need to be consec\n",
    "        \n",
    "        batch_states =   T.Tensor([i[0] for i in minibatch]).to(self.Q_eval.device)\n",
    "        batch_actions =  T.LongTensor([i[1] for i in minibatch]).to(self.Q_eval.device)\n",
    "        batch_rewards =  T.Tensor([i[2] for i in minibatch]).to(self.Q_eval.device)\n",
    "        batch_next_states = T.Tensor([i[3] for i in minibatch]).to(self.Q_eval.device)\n",
    "        rewards = T.Tensor(batch_rewards).to(self.Q_eval.device)\n",
    "        \n",
    "        # Get predictions from both networks\n",
    "        Qpred = self.Q_eval.forward(batch_states).to(self.Q_eval.device)\n",
    "        Qnext = self.Q_next.forward(batch_next_states).to(self.Q_next.device)\n",
    "        \n",
    "        max_Q_next, max_action = T.max(Qnext, dim=1)\n",
    "        \n",
    "        predictions = Qpred.gather(1, batch_actions.view(-1, 1)).squeeze()\n",
    "\n",
    "        # We want loss function to be zero for all actions except max - Video 2: 16:30ish\n",
    "        targets = rewards + self.gamma*max_Q_next \n",
    "        \n",
    "        # Get loss and backprob\n",
    "        loss = self.Q_eval.loss(targets, predictions).to(self.Q_eval.device)\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        return loss.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initial_fill_mem(agent, pretrain_length, env):\n",
    "    \"\"\"\n",
    "    Start filling the memory of the agent with a bunch of random moves and transitions to start learning from.\n",
    "    \n",
    "    Pretrain length should be at least 1 batchsize\n",
    "    \"\"\"\n",
    "    # Initialize deque with zero-images one array for each image\n",
    "    stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "    \n",
    "    possible_actions = agent.action_space\n",
    "    \n",
    "    for i in range(pretrain_length):\n",
    "        \n",
    "        # If it's the first step, reset the environment\n",
    "        if i == 0:\n",
    "            frame = env.reset()\n",
    "\n",
    "            state, stacked_frames = stack_frames(stacked_frames, frame, is_new_episode=True)\n",
    "\n",
    "        # Get the next_state, the rewards, done by taking a random action\n",
    "        action = np.random.choice(possible_actions)\n",
    "        next_frame, reward, done, info = env.step(action)\n",
    "\n",
    "        if episode_render: env.render()\n",
    "\n",
    "        # Stack the frames\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_frame, is_new_episode=False)\n",
    "\n",
    "\n",
    "        # If the episode is finished (we're dead 3x), then store a 0 next state and restart the env.\n",
    "        if done:\n",
    "            # We finished the episode\n",
    "            next_state = np.zeros(state.shape)\n",
    "\n",
    "            # Add experience to memory\n",
    "            agent.storeTransition(state, action, reward, next_state)\n",
    "\n",
    "            # Start a new episode\n",
    "            frame = env.reset()\n",
    "\n",
    "            # Stack the frames\n",
    "            state, stacked_frames = stack_frames(stacked_frames, frame, is_new_episode=True)\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            agent.storeTransition(state, action, reward, next_state)\n",
    "\n",
    "            # Our new state is now the next_state\n",
    "            state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "frame_shape = [110, 84]\n",
    "state_size = [110, 84, 4]         # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
    "alpha =  0.00025          # Alpha (aka learning rate)\n",
    "action_space = range(4)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 50            # Total episodes for training\n",
    "max_steps = 50000              # Max possible steps in an episode\n",
    "batch_size = 64                # Batch size\n",
    "target_net_replace = 10000     # Target network replacement\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "epsilon_start = 1.0            # exploration probability at start\n",
    "epsilon_end = 0.01            # minimum exploration probability \n",
    "epsilon_decay = 0.00001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.9                    # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000 #00        # Number of experiences the Memory can keep\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4                 # Number of frames stacked\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(frame_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "              alpha, max_memory, target_net_replace, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "initial_fill_mem(agent, 128, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026480110827833414\n",
      "0.0021692118607461452\n",
      "0.0026270283851772547\n",
      "0.0025744037702679634\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0100\n",
      "0.002773401327431202\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0128\n",
      "0.0029585431329905987\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9997 Training Loss 0.0158\n",
      "0.002628103829920292\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n",
      "Episode: 0 Total reward: 55.0 Explore P: 0.9996 Training Loss 0.0184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-497-6eda59f4729f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_new_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Set step = max_steps to end the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f38d9d5b63c8>\u001b[0m in \u001b[0;36mstack_frames\u001b[0;34m(stacked_frames, state, is_new_episode, stack_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# https://github.com/openai/gym/issues/275 - Env is already skipping frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Stacked_frames = the queue, stacked_state = an array of the same information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_new_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-81adb582f255>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Thanks to Mikołaj Walkowiak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpreprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessed_frame\u001b[0m \u001b[0;31m# 110x84x1 frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         image = ndi.gaussian_filter(image, anti_aliasing_sigma,\n\u001b[0;32m--> 149\u001b[0;31m                                     cval=cval, mode=ndi_mode)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# 2-dimensional interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0;32m--> 289\u001b[0;31m                               mode, cval, truncate)\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mlw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36m_gaussian_kernel1d\u001b[0;34m(sigma, order, radius)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order must be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mphi_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/polynomial/_polybase.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, coef, domain, window)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/polynomial/polyutils.py\u001b[0m in \u001b[0;36mas_series\u001b[0;34m(alist, trim)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coefficient array is empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coefficient array is not 1-d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model! - Still needs work\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    # Set episode_steps to 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    # Initialize the rewards of the episode\n",
    "    episode_rewards = [] \n",
    "    episode_losses = []\n",
    "\n",
    "    # Make a new episode and observe the first state\n",
    "    frame = env.reset()\n",
    "\n",
    "    # Remember that stack frame function also call our preprocess function.\n",
    "    state, stacked_frames = stack_frames(stacked_frames, frame, is_new_episode=True)\n",
    "\n",
    "    while episode_steps < max_steps:\n",
    "        episode_steps += 1\n",
    "\n",
    "        # Predict the action to take and take it\n",
    "        action, explore_probability = agent.chooseAction(state)\n",
    "        \n",
    "        #Perform the action and get the next_state, reward, and done information\n",
    "        next_frame, reward, done, info = env.step(action)\n",
    "\n",
    "        if episode_render:\n",
    "            env.render()\n",
    "\n",
    "        # Add the reward to total reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # If the game is finished\n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_frame, is_new_episode=False)\n",
    "\n",
    "            # Set step = max_steps to end the episode\n",
    "            step = max_steps\n",
    "\n",
    "            # Get the total reward of the episode\n",
    "            total_reward = sum(episode_rewards)\n",
    "            total_loss = sum(episode_losses)\n",
    "\n",
    "            print('Episode: {}'.format(episode),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_probability),\n",
    "                  'Training Loss {:.4f}'.format(total_loss))\n",
    "\n",
    "            # Store transition <st,at,rt+1,st+1> in memory D\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        else:\n",
    "            # Stack the frame of the next_state\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_frame, is_new_episode=False)\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            # st+1 is now our current state\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        ### LEARNING PART   \n",
    "        if episode_steps % 100 == 0:\n",
    "            loss = agent.learn(batch_size)\n",
    "            episode_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
