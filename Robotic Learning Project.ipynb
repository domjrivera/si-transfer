{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from gym.envs.classic_control import rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def repeat_upsample(rgb_array, k=1, l=1, err=[]):\n",
    "    # repeat kinda crashes if k/l are zero\n",
    "    if k <= 0 or l <= 0: \n",
    "        if not err: \n",
    "            print(\"Number of repeats must be larger than 0, k: {}, l: {}, returning default array!\".format(k, l))\n",
    "            err.append('logged')\n",
    "        return rgb_array\n",
    "\n",
    "    # repeat the pixels k times along the y axis and l times along the x axis\n",
    "    # if the input image is of shape (m,n,3), the output image will be of shape (k*m, l*n, 3)\n",
    "\n",
    "    return np.repeat(np.repeat(rgb_array, k, axis=0), l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "viewer_S = rendering.SimpleImageViewer()\n",
    "viewer_A = rendering.SimpleImageViewer()\n",
    "viewer_D = rendering.SimpleImageViewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_S = gym.make('SpaceInvaders-v0')\n",
    "print(\"Action Space: \", env_S.action_space)\n",
    "print(\"Observation Space: \", env_S.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(7)\n",
      "Observation Space:  Box(250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_A = gym.make('Assault-v0')\n",
    "print(\"Action Space: \", env_A.action_space)\n",
    "print(\"Observation Space: \", env_A.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_D = gym.make('DemonAttack-v0')\n",
    "print(\"Action Space: \", env_D.action_space)\n",
    "print(\"Observation Space: \", env_D.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent_all():\n",
    "    def __init__(self, env):\n",
    "        # 0 = nothing, 1 = fire, 2 = right, 3 = left\n",
    "        self.common_actions = [0, 1, 2, 3]\n",
    "        self.assult_translation = {0:1, 1:2, 2:3, 3:4}\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        base_action = random.choice(self.common_actions)\n",
    "        \n",
    "        assault_action = self.assult_translation[base_action]\n",
    "        \n",
    "        return base_action, base_action, assault_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_S = env_S.reset()\n",
    "state_A = env_A.reset()\n",
    "state_D = env_D.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent = Agent_all(env_S)\n",
    "\n",
    "for _ in range(2000):\n",
    "    action_S, action_D, action_A = agent.get_action(state_S)\n",
    "    \n",
    "    state_S, reward_S, done_S, info_S = env_S.step(action_S)\n",
    "    \n",
    "    rgb_S = env_S.render('rgb_array')\n",
    "    rgb_S = rgb_S[22:195, :, :]\n",
    "    viewer_S.imshow(repeat_upsample(rgb_S, 3, 3))\n",
    "    \n",
    "    state_A, reward_A, done_A, info_A = env_A.step(action_A)\n",
    "    \n",
    "    rgb_A = env_A.render('rgb_array')\n",
    "    rgb_A = rgb_A[51:195+29, :, :]\n",
    "    viewer_A.imshow(repeat_upsample(rgb_A, 3, 3))\n",
    "    \n",
    "    state_D, reward_D, done_D, info_D = env_D.step(action_D)\n",
    "    \n",
    "    rgb_D = env_D.render('rgb_array')\n",
    "    rgb_D = rgb_D[15:188, :, :]\n",
    "    viewer_D.imshow(repeat_upsample(rgb_D, 3, 3))\n",
    "    \n",
    "    #time.sleep(0.1)\n",
    "    if done_S & done_A & done_D:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 160)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_A.mean(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "SI: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  \n",
    "A:  0 = nothing, 1 = nothing?, 2 = fire, 3 = right, 4 = left, 5 = shoot right, 6 = shoot left  \n",
    "DA: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tutorial\n",
    "Full code, but using tensorflow\n",
    " - https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb  \n",
    " - https://www.youtube.com/watch?v=gCJyVX98KJ4\n",
    "\n",
    "Using Pytorch\n",
    " - https://www.youtube.com/watch?v=RfNxXlO6BiA\n",
    "\n",
    "Also\n",
    " - https://www.youtube.com/watch?v=dpBKz1wxE_c\n",
    " \n",
    "Frame Skipping and Stacking\n",
    " - https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray # Help us to gray our frames\n",
    "\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "print(\"Action Space: \", env.action_space)\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
    "#possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "# - His env takes a one hot vector as the action. Ours just takes an integer\n",
    "\n",
    "possible_actions = range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame \n",
    "    gray = rgb2gray(frame)\n",
    "    \n",
    "    # Crop the screen (remove the part below the player)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = gray[8:-12,4:-12]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    # Thanks to Mikołaj Walkowiak\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
    "    \n",
    "    return preprocessed_frame # 110x84x1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode, stack_size=4):\n",
    "    # Preprocess frame\n",
    "    # https://github.com/openai/gym/issues/275 - Env is already skipping frames\n",
    "    # Stacked_frames = the queue, stacked_state = an array of the same information\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for i in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "        \n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size, consecutive=False):\n",
    "        buffer_size = len(self.buffer)\n",
    "        \n",
    "        if consecutive:\n",
    "            rand_start = np.random.choice(range(buffer_size - batch_size - 1))\n",
    "            sample = self.buffer[rand_start:rand_start + batch_size]\n",
    "        else:\n",
    "            random_indices = np.random.choice(np.arange(buffer_size),\n",
    "                                    size = batch_size,\n",
    "                                    replace = False)\n",
    "            sample = [self.buffer[i] for i in random_indices]\n",
    "        \n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    \"\"\"\n",
    "    Number of Linear input connections depends on output of conv2d layers\n",
    "    and therefore the input image size, so compute it.\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, frame_shape, num_outputs, alpha):\n",
    "        super(DQN, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        h, w = frame_shape\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        self.linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_outputs)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=alpha)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization.\n",
    "    def forward(self, x):\n",
    "        x = T.Tensor(x).to(self.device)\n",
    "        \n",
    "        # Must transpose it - comes in as (h, w, frames). Change to (frames, h, w)\n",
    "        x = x.view(-1, 1, 110, 84)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten \n",
    "        x = x.view(-1, self.linear_input_size)\n",
    "        \n",
    "        # Linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        actions = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Actions is a q value for each action, for each frame. \n",
    "        \n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, frame_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "                 alpha, max_memory, replace, action_space):\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon_start = epsilon_start # For greedy action selection\n",
    "        self.epsilon_end = epsilon_end #How low epsilon can go\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.replace = replace # How often to replace target network\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0 #How many times we've called learn function - For target network replacement\n",
    "        self.memory = Memory(max_memory)\n",
    "        \n",
    "        self.replace_target = replace\n",
    "        \n",
    "        self.Q_eval = DQN(frame_shape, len(action_space), alpha) # Agent's estimate of current set of states' Q\n",
    "        self.Q_next = DQN(frame_shape, len(action_space), alpha) # Agent's estimage of next set of states' Q\n",
    "        \n",
    "        \n",
    "    def storeTransition(self, state, action, reward, result_state):\n",
    "        \"\"\"\n",
    "        Wrap the inputs into a \"Transition\" and puts in in memory \n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, result_state)\n",
    "        self.memory.add(transition)\n",
    "        \n",
    "        \n",
    "    def chooseAction(self, observation):\n",
    "        \"\"\"\n",
    "        Epsilon Greedy strategy with decaying exploration prob.\n",
    "        \"\"\"\n",
    "        rand = np.random.rand()\n",
    "        \n",
    "        # Lower the exploration probability over time in accordance with \n",
    "        #  the number of times the model has learned \n",
    "        explore_probability = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                              np.exp(-self.epsilon_decay * self.learn_step_counter)\n",
    "        \n",
    "        if rand < explore_probability:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            actions_pred = self.Q_eval.forward(observation)\n",
    "                 # Take first axis because actions returned as a matrix\n",
    "            action = T.argmax(actions_pred[1]).item()\n",
    "            \n",
    "        self.steps += 1\n",
    "            \n",
    "        return action, explore_probability\n",
    "    \n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        \"\"\"\n",
    "        Batch learn\n",
    "        \"\"\"\n",
    "        # Zero gradients for next batch\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        # Target network replacement\n",
    "        if (self.replace_target is not None) & (self.learn_step_counter % self.replace_target == 0):\n",
    "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "            \n",
    "        # Get a minibatch\n",
    "        minibatch = self.Memory.sample(batch_size, consecutive=True) # Not sure they need to be conseq\n",
    "        this_batch = np.array(minibatch)\n",
    "        \n",
    "        # Get predictions from both networks\n",
    "        # A list of an array to appease pytorch\n",
    "        Qpred = self.Q_eval.forward(list(this_batch[:, 0][:])).to(self.Q_eval.device)\n",
    "        Qnext = self.Q_next.forward(list(this_batch[:, 3][:])).to(self.Q_next.device)\n",
    "        \n",
    "        maxAction = T.argmax(Qnext, dim=1).to(self.Q_next.device)\n",
    "        rewards  = T.Tensor(list(this_batch[:, 2])).to(self.Q_eval.device)\n",
    "\n",
    "        # We want loss function to be zero for all actions except max - Video 2: 16:30ish\n",
    "        Qtarget = Qpred   \n",
    "        Qtarget[:, maxAction] = rewards + self.gamma*T.max(Qnext[1]) \n",
    "        \n",
    "        # Get loss and backprob\n",
    "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.learn_step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "frame_shape = [110, 84]\n",
    "state_size = [110, 84, 4]         # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
    "action_size = env.action_space.n  # 8 possible actions\n",
    "alpha =  0.00025          # Alpha (aka learning rate)\n",
    "action_space = range(4)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 50            # Total episodes for training\n",
    "max_steps = 50000              # Max possible steps in an episode\n",
    "batch_size = 64                # Batch size\n",
    "target_net_replace = 10000     # Target network replacement\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "epsilon_start = 1.0            # exploration probability at start\n",
    "epsilon_end = 0.01            # minimum exploration probability \n",
    "epsilon_decay = 0.00001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.9                    # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10000 #00        # Number of experiences the Memory can keep\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4                 # Number of frames stacked\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Fill memory with first few frames of the game to get a full batch\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    # Get the next_state, the rewards, done by taking a random action\n",
    "    action = np.random.choice(possible_actions)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if episode_render: env.render()\n",
    "    \n",
    "    # Stack the frames\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    \n",
    "    \n",
    "    # If the episode is finished (we're dead 3x)\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our new state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(frame_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "              alpha, max_memory, target_net_replace, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train the Model! - Still needs work\n",
    "\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    # Set episode_steps to 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    # Initialize the rewards of the episode\n",
    "    episode_rewards = [] \n",
    "\n",
    "    # Make a new episode and observe the first state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Remember that stack frame function also call our preprocess function.\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while episode_steps < max_steps:\n",
    "        episode_steps += 1\n",
    "\n",
    "        # Predict the action to take and take it\n",
    "        action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "        #Perform the action and get the next_state, reward, and done information\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if episode_render:\n",
    "            env.render()\n",
    "\n",
    "        # Add the reward to total reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # If the game is finished\n",
    "        if done:\n",
    "            # The episode ends so no next state\n",
    "            next_state = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            # Set step = max_steps to end the episode\n",
    "            step = max_steps\n",
    "\n",
    "            # Get the total reward of the episode\n",
    "            total_reward = np.sum(episode_rewards)\n",
    "\n",
    "            print('Episode: {}'.format(episode),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                         'Explore P: {:.4f}'.format(explore_probability),\n",
    "                      'Training Loss {:.4f}'.format(loss))\n",
    "\n",
    "            rewards_list.append((episode, total_reward))\n",
    "\n",
    "            # Store transition <st,at,rt+1,st+1> in memory D\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        else:\n",
    "            # Stack the frame of the next_state\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            # st+1 is now our current state\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        ### LEARNING PART            \n",
    "        # Obtain random mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "        actions_mb = np.array([each[1] for each in batch])\n",
    "        rewards_mb = np.array([each[2] for each in batch]) \n",
    "        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "        dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "        target_Qs_batch = []\n",
    "\n",
    "        # Get Q values for next_state \n",
    "        Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "\n",
    "        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "        for i in range(0, len(batch)):\n",
    "            terminal = dones_mb[i]\n",
    "\n",
    "            # If we are in a terminal state, only equals reward\n",
    "            if terminal:\n",
    "                target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "            else:\n",
    "                target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                target_Qs_batch.append(target)\n",
    "\n",
    "\n",
    "        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "        loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                           DQNetwork.target_Q: targets_mb,\n",
    "                                           DQNetwork.actions_: actions_mb})\n",
    "\n",
    "        # Write TF Summaries\n",
    "        summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "        writer.add_summary(summary, episode)\n",
    "        writer.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
