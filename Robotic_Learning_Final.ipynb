{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "rsL_7JHVnC74"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "#from gym.envs.classic_control import rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "1TaRn7CjnC8F"
   },
   "outputs": [],
   "source": [
    "def repeat_upsample(rgb_array, k=1, l=1, err=[]):\n",
    "    # repeat kinda crashes if k/l are zero\n",
    "    if k <= 0 or l <= 0: \n",
    "        if not err: \n",
    "            print(\"Number of repeats must be larger than 0, k: {}, l: {}, returning default array!\".format(k, l))\n",
    "            err.append('logged')\n",
    "        return rgb_array\n",
    "\n",
    "    # repeat the pixels k times along the y axis and l times along the x axis\n",
    "    # if the input image is of shape (m,n,3), the output image will be of shape (k*m, l*n, 3)\n",
    "\n",
    "    return np.repeat(np.repeat(rgb_array, k, axis=0), l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "48bQI8g3nC8L"
   },
   "outputs": [],
   "source": [
    "viewer_S = rendering.SimpleImageViewer()\n",
    "viewer_A = rendering.SimpleImageViewer()\n",
    "viewer_D = rendering.SimpleImageViewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "tkuJa9dNnC8R",
    "outputId": "6eb1c3c8-89b3-44f3-cb83-a7e47a4132d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_S = gym.make('SpaceInvaders-v0')\n",
    "print(\"Action Space: \", env_S.action_space)\n",
    "print(\"Observation Space: \", env_S.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "g75twpN6nC8Z",
    "outputId": "06cae319-826d-4c52-adc4-d25e6051051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(7)\n",
      "Observation Space:  Box(250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_A = gym.make('Assault-v0')\n",
    "print(\"Action Space: \", env_A.action_space)\n",
    "print(\"Observation Space: \", env_A.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "AGd2vcbwnC8f",
    "outputId": "a159c28e-23fd-4c5a-9df4-4c22b1084d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(6)\n",
      "Observation Space:  Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env_D = gym.make('DemonAttack-v0')\n",
    "print(\"Action Space: \", env_D.action_space)\n",
    "print(\"Observation Space: \", env_D.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "2NYhGj8wnC8m"
   },
   "outputs": [],
   "source": [
    "class Agent_all():\n",
    "    def __init__(self, env):\n",
    "        # 0 = nothing, 1 = fire, 2 = right, 3 = left\n",
    "        self.common_actions = [0, 1, 2, 3]\n",
    "        self.assult_translation = {0:1, 1:2, 2:3, 3:4}\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        base_action = random.choice(self.common_actions)\n",
    "        \n",
    "        assault_action = self.assult_translation[base_action]\n",
    "        \n",
    "        return base_action, base_action, assault_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "8j7dbrjFnC8r"
   },
   "outputs": [],
   "source": [
    "state_S = env_S.reset()\n",
    "state_A = env_A.reset()\n",
    "state_D = env_D.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VFpRHXaUnC8v"
   },
   "outputs": [],
   "source": [
    "agent = Agent_all(env_S)\n",
    "\n",
    "for _ in range(2000):\n",
    "    action_S, action_D, action_A = agent.get_action(state_S)\n",
    "    \n",
    "    state_S, reward_S, done_S, info_S = env_S.step(action_S)\n",
    "    \n",
    "    rgb_S = env_S.render('rgb_array')\n",
    "    rgb_S = rgb_S[22:195, :, :]\n",
    "    viewer_S.imshow(repeat_upsample(rgb_S, 3, 3))\n",
    "    \n",
    "    state_A, reward_A, done_A, info_A = env_A.step(action_A)\n",
    "    \n",
    "    rgb_A = env_A.render('rgb_array')\n",
    "    rgb_A = rgb_A[51:195+29, :, :]\n",
    "    viewer_A.imshow(repeat_upsample(rgb_A, 3, 3))\n",
    "    \n",
    "    state_D, reward_D, done_D, info_D = env_D.step(action_D)\n",
    "    \n",
    "    rgb_D = env_D.render('rgb_array')\n",
    "    rgb_D = rgb_D[15:188, :, :]\n",
    "    viewer_D.imshow(repeat_upsample(rgb_D, 3, 3))\n",
    "    \n",
    "    #time.sleep(0.1)\n",
    "    if done_S & done_A & done_D:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "bZT8TcxinC81",
    "outputId": "8fe3d9f4-064e-4817-8933-d8e508e0fbf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 160)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_A.mean(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "aLtvRhL5nC89"
   },
   "source": [
    "SI: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  \n",
    "A:  0 = nothing, 1 = nothing?, 2 = fire, 3 = right, 4 = left, 5 = shoot right, 6 = shoot left  \n",
    "DA: 0 = nothing, 1 = fire, 2 = right, 3 = left, 4 = right + fire, 5 = left + fire  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "vfG1tF8UnC8-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "CwVeNH2jnC9E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "pZgFbkOknC9H"
   },
   "source": [
    "# Tutorial\n",
    "Full code, but using tensorflow\n",
    " - https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Space%20Invaders/DQN%20Atari%20Space%20Invaders.ipynb  \n",
    " - https://www.youtube.com/watch?v=gCJyVX98KJ4\n",
    "\n",
    "Using Pytorch\n",
    " - https://www.youtube.com/watch?v=RfNxXlO6BiA\n",
    "\n",
    "Also\n",
    " - https://www.youtube.com/watch?v=dpBKz1wxE_c\n",
    " \n",
    "Frame Skipping and Stacking\n",
    " - https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    " \n",
    "General DQN Walkthrough =\n",
    " - https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "9wi-qqj8nC9J"
   },
   "outputs": [],
   "source": [
    "import numpy as np           # Handle matrices\n",
    "\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray # Help us to gray our frames\n",
    "\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "w8_TtqncnC9N"
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import atari_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "rm-KPHa5wcow"
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "VsEH8smcnC9h"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        \n",
    "        random_indices = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        sample = [self.buffer[i] for i in random_indices]\n",
    "        \n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "jZxwMr1rnC9l"
   },
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    \"\"\"\n",
    "    Number of Linear input connections depends on output of conv2d layers\n",
    "    and therefore the input image size, so compute it.\n",
    "    \"\"\"\n",
    "    out_size = (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "    return out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "lyaeouS6nC9q"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, frame_shape, num_outputs, alpha):\n",
    "        super(DQN, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        h, w, d = frame_shape\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        self.linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_outputs)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization.\n",
    "    def forward(self, x):\n",
    "        batch_shape = x.shape\n",
    "        x = x/255.0\n",
    "        \n",
    "        # Must transpose it - comes in as (h, w, frames). Change to (frames, h, w)\n",
    "        x = x.view(batch_shape[0], batch_shape[3], batch_shape[1], batch_shape[2])\n",
    "\n",
    "        \n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten \n",
    "        x = x.view(-1, self.linear_input_size)\n",
    "        \n",
    "        # Linear layers\n",
    "        x = F.elu(self.fc1(x))\n",
    "        q_vals = self.fc2(x)\n",
    "        \n",
    "        # q_vals is a q value for each action for each item in minibatch (Eg: 32 * 4)\n",
    "        \n",
    "        return q_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "ciGAPeBInC9t"
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, frame_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "                 alpha, max_memory, replace, action_space):\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon_start = epsilon_start # For greedy action selection\n",
    "        self.epsilon_end = epsilon_end #How low epsilon can go\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.replace = replace # How often to replace target network\n",
    "        self.action_space = action_space\n",
    "        self.n_actions = len(action_space)\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.learn_step_counter = 0 #How many times we've called learn function - For target network replacement\n",
    "        self.memory = Memory(max_memory)\n",
    "        \n",
    "        self.replace_target = replace\n",
    "        \n",
    "        self.Q_eval = DQN(frame_shape, len(action_space), alpha) # Agent's estimate of current set of states' Q\n",
    "        self.Q_next = DQN(frame_shape, len(action_space), alpha) # Agent's estimage of next set of states' Q\n",
    "        \n",
    "        self.action_values = T.Tensor(action_space).to(self.Q_eval.device)\n",
    "        \n",
    "        \n",
    "    def storeTransition(self, state, action, reward, result_state, done):\n",
    "        \"\"\"\n",
    "        Wrap the inputs into a \"Transition\" and puts in in memory \n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, result_state, done)\n",
    "        self.memory.add(transition)\n",
    "        \n",
    "        \n",
    "    def chooseAction(self, observation, explore_bool=True):\n",
    "        \"\"\"\n",
    "        Epsilon Greedy strategy with decaying exploration prob.\n",
    "        \"\"\"\n",
    "        rand = np.random.rand()\n",
    "        \n",
    "        if not explore_bool:\n",
    "            explore_probability = 0\n",
    "            \n",
    "        else:\n",
    "            # Lower the exploration probability over time in accordance with \n",
    "            #  the number of times the model has learned \n",
    "            explore_probability = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                                  np.exp(-self.epsilon_decay * self.learn_step_counter)\n",
    "        \n",
    "        if rand < explore_probability:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            observation = T.Tensor(np.array(observation)).unsqueze(0).to(self.Q_eval.device)\n",
    "            actions_pred = self.Q_eval.forward(observation)\n",
    "                 \n",
    "            # Take first axis because actions returned as a matrix (1 x num_actions) \n",
    "            action = T.argmax(actions_pred[0]).item()\n",
    "            \n",
    "        self.steps += 1\n",
    "            \n",
    "        return action, explore_probability\n",
    "    \n",
    "    \n",
    "    def initial_fill_mem(self, env, pretrain_length, episode_render, obs_shape):\n",
    "        \"\"\"\n",
    "        Start filling the memory of the agent with a bunch of random moves and transitions to start learning from.\n",
    "        \"\"\"    \n",
    "        state = env.reset()\n",
    "\n",
    "        for i in range(pretrain_length):\n",
    "\n",
    "            # Get the next_state and the reward, done by taking a random action\n",
    "            action = np.random.choice(self.action_space)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if episode_render: env.render()\n",
    "\n",
    "            # If the episode is finished (we're dead 3x), then store a 0 next state and restart the env.\n",
    "            if not done:\n",
    "                # Add experience to memory\n",
    "                self.storeTransition(state, action, reward, next_state, done)\n",
    "\n",
    "                # Our new state is now the next_state\n",
    "                state = next_state\n",
    "                \n",
    "            else: \n",
    "                # We finished the episode, add a state of all black\n",
    "                next_state = np.zeros(obs_shape)\n",
    "\n",
    "                # Add experience to memory\n",
    "                self.storeTransition(state, action, reward, next_state, done)\n",
    "\n",
    "                # Start a new episode\n",
    "                state = env.reset()\n",
    "                \n",
    "                \n",
    "    def run_episode(self, env, learn_bool, explore_bool, max_steps, obs_shape, batch_size):\n",
    "        \"\"\"\n",
    "        Run a single episode\n",
    "        \"\"\"\n",
    "        # Make a new episode and observe the first state\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        done = False\n",
    "\n",
    "        while (episode_steps < max_steps) & (~done):\n",
    "            episode_steps += 1\n",
    "\n",
    "            # Predict the action to take and take it\n",
    "            action, explore_probability = agent.chooseAction(state, explore_bool)\n",
    "\n",
    "            #Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if episode_render: env.render()\n",
    "\n",
    "            # Add the reward to total reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # If it terminated, add a blank screen as the next state\n",
    "            if done: next_state = np.zeros(obs_shape, dtype=np.int)\n",
    "\n",
    "            # Add experience to memory\n",
    "            self.storeTransition(state, action, reward, next_state, done)\n",
    "\n",
    "            # st+1 is now our current state\n",
    "            state = next_state\n",
    "\n",
    "            ### LEARNING PART  \n",
    "            if learn_bool:\n",
    "                loss = agent.learn(batch_size)\n",
    "                episode_losses.append(loss)\n",
    "\n",
    "\n",
    "        # Get the mean loss of the episode\n",
    "        mean_loss = sum(episode_losses)/len(episode_losses)\n",
    "\n",
    "        print('Episode: {}'.format(episode),\n",
    "              'Total reward: {}'.format(total_reward),\n",
    "              'Explore P: {:.4f}'.format(explore_probability),\n",
    "              'Mean Training Loss {:.4f}'.format(mean_loss))\n",
    "\n",
    "        return episode_reward, mean_loss, explore_probability\n",
    "    \n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        \"\"\"\n",
    "        Batch learn\n",
    "        \"\"\"\n",
    "        # Zero gradients for next batch\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        # Target network replacement\n",
    "        if self.replace_target is not None:\n",
    "            if self.learn_step_counter % self.replace_target == 0:\n",
    "                print('replacing target')\n",
    "                self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
    "            \n",
    "        # Get a minibatch\n",
    "        minibatch = self.memory.sample(batch_size)\n",
    "        \n",
    "        batch_states =   T.stack([T.Tensor(np.array(i[0])).to(self.Q_eval.device) for i in minibatch])\n",
    "        batch_actions =  T.LongTensor([i[1] for i in minibatch]).to(self.Q_eval.device)\n",
    "        batch_rewards =  T.Tensor([i[2] for i in minibatch]).to(self.Q_eval.device)\n",
    "        batch_next_states = T.stack([T.Tensor(np.array(i[3])).to(self.Q_eval.device) for i in minibatch])\n",
    "        done_bools =  T.Tensor([i[4] for i in minibatch]).to(self.Q_eval.device)\n",
    "        \n",
    "        batch_index = T.arange(batch_size, dtype=T.long, device=self.Q_eval.device)\n",
    "        \n",
    "        # Get predictions from both networks\n",
    "        Qpred  = self.Q_eval.forward(batch_states).to(self.Q_eval.device)\n",
    "        target = self.Q_eval.forward(batch_states).to(self.Q_eval.device)\n",
    "        Qnext = self.Q_next.forward(batch_next_states).to(self.Q_next.device)\n",
    "        \n",
    "        target[batch_index, batch_actions] = batch_rewards + \\\n",
    "              self.gamma*T.max(Qnext, dim=1)[0]*done_bools\n",
    "            \n",
    "        print('target', target.sum())\n",
    "      \n",
    "        \n",
    "        # Get loss and backprob\n",
    "        loss = self.Q_eval.loss(target, Qpred).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        print('loss', loss)\n",
    "        \n",
    "        return loss.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "96k-VX5XnC95"
   },
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "ulQSfHJmnC96"
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "alpha =  0.00025                   # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000           # Total episodes for training\n",
    "max_steps = 50000              # Max possible steps in an episode\n",
    "batch_size = 64                # Batch size\n",
    "target_net_replace = 50000     # Target network replacement\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "epsilon_start = 1.0            # exploration probability at start\n",
    "epsilon_end = 0.01            # minimum exploration probability \n",
    "epsilon_decay = 0.00001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.9                    # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length =  10000         # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000           # Number of experiences the Memory can keep\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4                 # Number of frames stacked\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  range(0, 6)\n",
      "Observation Space:  (84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create our environment\n",
    "#env = atari_wrappers.make_atari('SpaceInvadersNoFrameskip-v0', max_steps)\n",
    "env = atari_wrappers.make_atari('PongNoFrameskip-v0', max_steps)\n",
    "env = atari_wrappers.wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=True, scale=False)\n",
    "\n",
    "action_space = range(env.action_space.n)\n",
    "obs_shape = env.observation_space.shape\n",
    "\n",
    "print(\"Action Space: \", action_space)\n",
    "print(\"Observation Space: \", obs_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "38JEBe1ynC98"
   },
   "outputs": [],
   "source": [
    "agent = Agent(obs_shape, gamma, epsilon_start, epsilon_end, epsilon_decay, \n",
    "              alpha, memory_size, target_net_replace, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "tQsn4Rl7naMk",
    "outputId": "e6fdd6f1-0ace-4ece-966a-e1132f916e2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q_eval.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "wqwWeoS6nC9_"
   },
   "outputs": [],
   "source": [
    "agent.initial_fill_mem(env, pretrain_length, episode_render, obs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "tYh30kJRnC-F",
    "outputId": "15471700-6ebd-4823-924c-3368da7b0602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(72,36;446.4x217.44)\n",
      "AxesImage(72,36;446.4x217.44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.0)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mem_element = -5\n",
    "\n",
    "fig=plt.figure(figsize=(8, 4))\n",
    "print(plt.imshow(np.array(agent.memory.buffer[mem_element][0]).sum(axis=2), cmap=plt.cm.gray))\n",
    "fig=plt.figure(figsize=(8, 4))\n",
    "print(plt.imshow(np.array(agent.memory.buffer[mem_element][3]).sum(axis=2), cmap=plt.cm.gray))\n",
    "agent.memory.buffer[mem_element][1], agent.memory.buffer[mem_element][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2370
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "-ejzSuednC-P",
    "outputId": "8d8d7092-a889-428f-a95a-5c2a017dd71f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing target\n",
      "target tensor(8.9253, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0116, grad_fn=<MeanBackward1>)\n",
      "target tensor(-165.8764, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0604, grad_fn=<MeanBackward1>)\n",
      "target tensor(-58.4413, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0233, grad_fn=<MeanBackward1>)\n",
      "target tensor(30.0722, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0100, grad_fn=<MeanBackward1>)\n",
      "target tensor(57.1181, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0277, grad_fn=<MeanBackward1>)\n",
      "target tensor(42.7580, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0188, grad_fn=<MeanBackward1>)\n",
      "target tensor(2.3853, grad_fn=<SumBackward0>)\n",
      "loss tensor(0.0150, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-615cd086a60e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     episode_reward, mean_loss, explore_probability = agent.run_episode(env, learn_bool=True, explore_bool=True,\n\u001b[0;32m----> 9\u001b[0;31m                                  max_steps=max_steps,  obs_shape=obs_shape, batch_size=batch_size)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-f9f485a4f9a1>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, env, learn_bool, explore_bool, max_steps, obs_shape, batch_size)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m### LEARNING PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlearn_bool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mepisode_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-f9f485a4f9a1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Get predictions from both networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mQpred\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mQnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-5b07a7284509>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model!\n",
    "all_losses = []\n",
    "all_rewards = []\n",
    "explore_probs = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    episode_reward, mean_loss, explore_probability = agent.run_episode(env, learn_bool=True, explore_bool=True,\n",
    "                                 max_steps=max_steps,  obs_shape=obs_shape, batch_size=batch_size)\n",
    "    \n",
    "    all_losses.append(mean_loss)\n",
    "    all_rewards.append(episode_reward)\n",
    "    explore_probs.append(explore_probability)\n",
    "    \n",
    "    \n",
    "    if (episode % 10 == 0) & (episode != 0):\n",
    "        fig, ax1 = plt.subplots()\n",
    "        avg_window = 8\n",
    "\n",
    "        ax1.set_xlabel('Episodes')\n",
    "        ax1.set_ylabel('Rewards', color='tab:red')\n",
    "        ax1.plot(range(avg_window-1, episode+1), running_mean(all_rewards, avg_window), color='tab:red')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        ax2.set_ylabel('Loss', color='tab:blue')  # we already handled the x-label with ax1\n",
    "        ax2.plot(range(avg_window-1, episode+1), running_mean(all_losses, avg_window), color='tab:blue')\n",
    "        ax2.plot(range(avg_window-1, episode+1), running_mean(explore_probs, avg_window))\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "NCe6oXs3nC-V"
   },
   "outputs": [],
   "source": [
    "with open('trained_new_loss.pkl', 'wb') as f:\n",
    "    pickle.dump(agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "I_P9nVCZ7BAN"
   },
   "outputs": [],
   "source": [
    "with open('trained.pkl', 'rb') as f:\n",
    "    trained500 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "qq7-GON-SCUI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Robotic Learning Final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
